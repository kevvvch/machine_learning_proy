{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in d:\\program files\\python\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in d:\\program files\\python\\lib\\site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\program files\\python\\lib\\site-packages (from imbalanced-learn->imblearn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\program files\\python\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in d:\\program files\\python\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\program files\\python\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\program files\\python\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, recall_score\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines\n",
    "#=================================================#\n",
    "IS_TRAIN_NOT_TEST = True\n",
    "\n",
    "ENV_LOCAL_MACHINE   = 1\n",
    "ENV_GOOGLE_COLLABS  = 2\n",
    "ENV_KAGGLE          = 3\n",
    "ENVIRONMENT = ENV_LOCAL_MACHINE\n",
    "\n",
    "#=================================================#\n",
    "CSV_DATASET_INPUT_TEST          = \"dataset_balanced_test.csv\"\n",
    "CSV_DATASET_INPUT_SMOTE_TRAIN   = \"dataset_balanced_smote_train.csv\"\n",
    "CSV_DATASET_INPUT_ADASYN_TRAIN  = \"dataset_balanced_adasyn_train.csv\"\n",
    "CSV_DATASET_INPUT_TOMEK_TRAIN   = \"dataset_balanced_tomek_train.csv\"\n",
    "CSV_DATASET_INPUT_ENN_TRAIN     = \"dataset_balanced_enn_train.csv\"\n",
    "\n",
    "if ENVIRONMENT == ENV_LOCAL_MACHINE:\n",
    "    PATH_DATASET_INPUT  = \"./dataset_t/\"\n",
    "if ENVIRONMENT == ENV_GOOGLE_COLLABS:\n",
    "    PATH_DATASET_INPUT  = \"drive/MyDrive/UTN_Finales/[F] Aprendizaje Automatico/Repositorio/dataset_t/\"\n",
    "\n",
    "FEATURE_TARGET = \"is_click\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 329045, 1: 87148})\n"
     ]
    }
   ],
   "source": [
    "# Import CSV\n",
    "if ENVIRONMENT == ENV_GOOGLE_COLLABS:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "dataset_train = pd.read_csv(PATH_DATASET_INPUT+CSV_DATASET_INPUT_TOMEK_TRAIN)\n",
    "\n",
    "X_train = dataset_train.drop(FEATURE_TARGET, axis=1)\n",
    "y_train = dataset_train[FEATURE_TARGET]\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 86393, 1: 6266})\n"
     ]
    }
   ],
   "source": [
    "dataset_test = pd.read_csv(PATH_DATASET_INPUT+CSV_DATASET_INPUT_TEST)\n",
    "\n",
    "X_test = dataset_test.drop(FEATURE_TARGET, axis=1)\n",
    "y_test = dataset_test[FEATURE_TARGET]\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {\n",
    "#    'criterion': ['entropy'],  # Criterion to measure the quality of a split\n",
    "#    'max_depth': [None, 5, 15, 30],  # Maximum depth of the tree\n",
    "#    'min_samples_split': [5, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "#    'min_samples_leaf': [5, 10, 15],  # Minimum number of samples required to be at a leaf node\n",
    "#    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "#}\n",
    "#\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "# \n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "# \n",
    "# # Make predictions with the best model\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'min_samples_split': np.int64(3), 'min_samples_leaf': np.int64(7), 'max_depth': np.int64(19), 'criterion': 'gini'}\n",
      "Best cross-validation score (weighted F1): 0.84\n",
      "Test set score: 0.92\n",
      "Test set F1 score (weighted): 0.89\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],  # criteria for splitting\n",
    "    'max_depth': np.arange(1, 20),  # maximum depth of the tree\n",
    "    'min_samples_split': np.arange(2, 20),  # minimum samples required to split an internal node\n",
    "    'min_samples_leaf': np.arange(1, 20)  # minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # number of parameter settings sampled\n",
    "    scoring='f1_weighted',  # evaluation metric\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best cross-validation score (weighted F1): {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test set score: {:.2f}\".format(test_score))\n",
    "\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test set F1 score (weighted): {:.2f}\".format(test_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     86393\n",
      "           1       0.08      0.02      0.03      6266\n",
      "\n",
      "    accuracy                           0.92     92659\n",
      "   macro avg       0.50      0.50      0.49     92659\n",
      "weighted avg       0.87      0.92      0.89     92659\n",
      "\n",
      "Confusion Matrix:\n",
      "[[85018  1375]\n",
      " [ 6153   113]]\n",
      "F1 Score: 0.02914624709827186\n",
      "Test set recall score (weighted): 0.92\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "test_recall_score = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test set recall score (weighted): {:.2f}\".format(test_recall_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
